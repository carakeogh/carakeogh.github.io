In this blog post, we are going to develop and assess a "fake news" classifier using Tensorflow. We will be using data accessed from [Kaggle](https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset) to accomplish this task.

As always, we start with our import statements. 


```python
import pandas as pd
import numpy as np
import tensorflow as tf
import re
import string

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras import losses

from tensorflow.keras.layers.experimental.preprocessing import TextVectorization
from tensorflow.keras.layers.experimental.preprocessing import StringLookup

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

from matplotlib import pyplot as plt
```

# 1. Acquire Training Data
We read in our data from a CSV file using `pd.readcsv()`. Upon examining or dataset, we can see that there are three important rows: `title`, `text`, and `fake`. `title` gives the title of the article, `text` is the full body of text from the article, and `fake` has a value of `1` fr if the article contains "fake news" and a value of `0` if the article is true. 



```python
train_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true"
```


```python
df = pd.read_csv(train_url)
df.head()
```





  <div id="df-ff407622-e27b-40f1-aeee-4de76ced23ed">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Unnamed: 0</th>
      <th>title</th>
      <th>text</th>
      <th>fake</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>17366</td>
      <td>Merkel: Strong result for Austria's FPO 'big c...</td>
      <td>German Chancellor Angela Merkel said on Monday...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5634</td>
      <td>Trump says Pence will lead voter fraud panel</td>
      <td>WEST PALM BEACH, Fla.President Donald Trump sa...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>17487</td>
      <td>JUST IN: SUSPECTED LEAKER and “Close Confidant...</td>
      <td>On December 5, 2017, Circa s Sara Carter warne...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>12217</td>
      <td>Thyssenkrupp has offered help to Argentina ove...</td>
      <td>Germany s Thyssenkrupp, has offered assistance...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5535</td>
      <td>Trump say appeals court decision on travel ban...</td>
      <td>President Donald Trump on Thursday called the ...</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-ff407622-e27b-40f1-aeee-4de76ced23ed')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>

  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-ff407622-e27b-40f1-aeee-4de76ced23ed button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-ff407622-e27b-40f1-aeee-4de76ced23ed');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>




# 2. Make a Dataset
Now we are going to construct a Tensorflow `Dataset` with a function `make_dataset(df)`. The first thing we do is remove stopwords from the article's `title` and `text`. Stopwords are words such as "the", "and", "but", etc. that are typically considered to be uninformative and could crowd our dataset with unnecessary information. The second thing we do is return `tf.data.Dataset` with two inputs (`title` and `text`) and one output (`fake`). In a tensorflow `Dataset`, we need to distinguish between our model inputs, so we two dictionaries. The first dictionary specifies the two components in the predictor data (`title` and `text`), and the second dictionary specifies the component of the target data (`fake`). We also use `dataset.batch(n)` in order to increase the speed of training, where `n` is the number of rows that is packaged into a chunk of data that the model is trained on.


```python
import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
```

    [nltk_data] Downloading package stopwords to /root/nltk_data...
    [nltk_data]   Unzipping corpora/stopwords.zip.





    True




```python
stop = stopwords.words('english')
```


```python
def make_dataset(df):
  #1. remove stopwards from the article text and title
  df["title"] = df["title"].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))
  df["text"]  = df["text"].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))

  #2. construct and return a tf.data.Dataset with 2 inputs and 1 output
  data = tf.data.Dataset.from_tensor_slices(
      (
          {
              "title" : df[["title"]],
              "text"  : df [["text"]]
          },
          {
              "fake"  : df[["fake"]]
          }
      )
  )

  #batch the Dataset
  data = data.batch(100)

  return data
```


```python
data = make_dataset(df)
```

After we make our `Dataset`, we will split it, with 20% for validation and 80% for training. We `shuffle` the data first in order avoid the model learning any ordered patterns in the data. 


```python
#Validation Data
data = data.shuffle(buffer_size = len(data))

#80% training data 
train_size = int(0.8*len(data))

#20% validation data
val_size   = int(0.2*len(data))

train = data.take(train_size)
val = data.skip(train_size).take(val_size)

#check that it is an 80/20 split
len(train), len(val)
```




    (180, 45)



We can also calculate the base rate, which is the accuracy of a model that would always make the same guess. We examine the labels on the training set to determine the base rate for this dataset.


```python
#Base Rate: FIX

labels_iterator = train.unbatch().map(lambda x, y: y["fake"]).as_numpy_iterator()

count = 0
for i in labels_iterator:
  if i == 1:
    count += 1
br = count/train_size


print("The baseline accuracy for the model is " + str(br) + "%")
```

    The baseline accuracy for the model is 52.1%


# 3. Create Models

We will now create three TensorFlow models to address the following question:

*When detecting fake news, is it most effective to focus on only the title of the article, the full text of the article, or both?*

In the first model, we will only use the article titles as the input. In the second model, we will only use the article texts as the input. In the third model, we will use both the article titles and the article texts as inputs.

We will use the Keras `Functional` API for constructing our models. We use this kind of Keras API because we have multiple kinds of input.

Before we construct our models, we need to do a few things. 

First, we need to standardize and vectorize our text. Standardization cleans up text data to make it more interpretable, such as removing capitals and punctuation. Vectorization represents text as a vector of the form (array, tensor), which makes the text interpretable to our model. 

Second, we need to specifiy two kinds of `keras.Input` for our model. We are treating the `text` and `title` information as two different kinds of data, even though they are both strings of text. We must specify the `shape`, `name`, and `dtype` of each. The `shape` is (1,) because the `text` and `title` columns each have one entry for each article. 


```python
#Text Preproccessing

size_vocabulary = 3000

def standardization(input_data):
    lowercase = tf.strings.lower(input_data)
    no_punctuation = tf.strings.regex_replace(lowercase,
                                  '[%s]' % re.escape(string.punctuation),'')
    return no_punctuation 

vectorize_layer = TextVectorization(
    standardize=standardization,
    max_tokens=size_vocabulary, # only consider this many words
    output_mode='int',
    output_sequence_length=500) 
```


```python
#Inputs

title_input = keras.Input(
    shape = (1,),
    name = "title",
    dtype = "string"
)

text_input = keras.Input(
    shape = (1,),
    name = "text",
    dtype = "string"
)
```

### Model 1: Article Title

We build a model with a pipeline of layers, similar to the way we built our model in Blog Post 5. We start with the vectorize layer to convert the input to a vector. One layer you might not have seen before is the `Embedding` layer. This layer lets us represents words in a vector space through word embedding. The idea is that in this vector space, words that are related will be close together and words that are not will be farther apart. The directions of the words could also be helpful as well. We give this layer a name so that we can use it to make a visualization later. We then create some more interrelated layers, of types you have seen before. Using `Dropout` helps us prevent overfitting.


```python
vectorize_layer.adapt(train.map(lambda input, output: input["title"]))

title_features = vectorize_layer(title_input)
title_features = layers.Embedding(size_vocabulary, 3, name = "embedding")(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.GlobalAveragePooling1D()(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.Dense(32, activation = "relu")(title_features)
```


```python
main = layers.Dense(32, activation='relu')(title_features)
output = layers.Dense(2, name = "fake")(main)
```

We create the model by specifying the input and the output, and then we can compile and plot the model's history to visualize the training and validation accuracy.


```python
model1 = keras.Model(
    inputs = [title_input],
    outputs = output
)
```


```python
model1.compile(optimizer = "adam",
               loss = losses.SparseCategoricalCrossentropy(from_logits=True),
               metrics=['accuracy']
)
```


```python
history = model1.fit(train, 
                    validation_data=val,
                    epochs = 30)
```

    Epoch 1/30


    /usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:559: UserWarning: Input dict contained keys ['text'] which did not match any model input. They will be ignored by the model.
      inputs = self._flatten_to_reference_inputs(inputs)


    180/180 [==============================] - 8s 20ms/step - loss: 0.6912 - accuracy: 0.5264 - val_loss: 0.6880 - val_accuracy: 0.5284
    Epoch 2/30
    180/180 [==============================] - 3s 16ms/step - loss: 0.6246 - accuracy: 0.7142 - val_loss: 0.4428 - val_accuracy: 0.9456
    Epoch 3/30
    180/180 [==============================] - 3s 16ms/step - loss: 0.2736 - accuracy: 0.9325 - val_loss: 0.1447 - val_accuracy: 0.9625
    Epoch 4/30
    180/180 [==============================] - 3s 17ms/step - loss: 0.1387 - accuracy: 0.9553 - val_loss: 0.0821 - val_accuracy: 0.9739
    Epoch 5/30
    180/180 [==============================] - 3s 18ms/step - loss: 0.1064 - accuracy: 0.9656 - val_loss: 0.0893 - val_accuracy: 0.9716
    Epoch 6/30
    180/180 [==============================] - 3s 14ms/step - loss: 0.0897 - accuracy: 0.9680 - val_loss: 0.0536 - val_accuracy: 0.9843
    Epoch 7/30
    180/180 [==============================] - 2s 10ms/step - loss: 0.0764 - accuracy: 0.9745 - val_loss: 0.0548 - val_accuracy: 0.9831
    Epoch 8/30
    180/180 [==============================] - 2s 9ms/step - loss: 0.0758 - accuracy: 0.9726 - val_loss: 0.0408 - val_accuracy: 0.9862
    Epoch 9/30
    180/180 [==============================] - 2s 9ms/step - loss: 0.0648 - accuracy: 0.9765 - val_loss: 0.0413 - val_accuracy: 0.9849
    Epoch 10/30
    180/180 [==============================] - 2s 9ms/step - loss: 0.0629 - accuracy: 0.9769 - val_loss: 0.0333 - val_accuracy: 0.9904
    Epoch 11/30
    180/180 [==============================] - 2s 10ms/step - loss: 0.0608 - accuracy: 0.9777 - val_loss: 0.0329 - val_accuracy: 0.9881
    Epoch 12/30
    180/180 [==============================] - 2s 12ms/step - loss: 0.0552 - accuracy: 0.9792 - val_loss: 0.0345 - val_accuracy: 0.9884
    Epoch 13/30
    180/180 [==============================] - 3s 15ms/step - loss: 0.0540 - accuracy: 0.9791 - val_loss: 0.0284 - val_accuracy: 0.9900
    Epoch 14/30
    180/180 [==============================] - 3s 17ms/step - loss: 0.0551 - accuracy: 0.9792 - val_loss: 0.0296 - val_accuracy: 0.9887
    Epoch 15/30
    180/180 [==============================] - 3s 16ms/step - loss: 0.0521 - accuracy: 0.9806 - val_loss: 0.0312 - val_accuracy: 0.9896
    Epoch 16/30
    180/180 [==============================] - 3s 15ms/step - loss: 0.0531 - accuracy: 0.9806 - val_loss: 0.0333 - val_accuracy: 0.9891
    Epoch 17/30
    180/180 [==============================] - 3s 16ms/step - loss: 0.0463 - accuracy: 0.9826 - val_loss: 0.0219 - val_accuracy: 0.9909
    Epoch 18/30
    180/180 [==============================] - 3s 15ms/step - loss: 0.0458 - accuracy: 0.9818 - val_loss: 0.0239 - val_accuracy: 0.9926
    Epoch 19/30
    180/180 [==============================] - 3s 17ms/step - loss: 0.0465 - accuracy: 0.9828 - val_loss: 0.0362 - val_accuracy: 0.9876
    Epoch 20/30
    180/180 [==============================] - 2s 10ms/step - loss: 0.0429 - accuracy: 0.9837 - val_loss: 0.0225 - val_accuracy: 0.9918
    Epoch 21/30
    180/180 [==============================] - 2s 10ms/step - loss: 0.0430 - accuracy: 0.9828 - val_loss: 0.0296 - val_accuracy: 0.9891
    Epoch 22/30
    180/180 [==============================] - 2s 10ms/step - loss: 0.0436 - accuracy: 0.9829 - val_loss: 0.0356 - val_accuracy: 0.9871
    Epoch 23/30
    180/180 [==============================] - 2s 10ms/step - loss: 0.0412 - accuracy: 0.9831 - val_loss: 0.0252 - val_accuracy: 0.9931
    Epoch 24/30
    180/180 [==============================] - 2s 14ms/step - loss: 0.0398 - accuracy: 0.9840 - val_loss: 0.0179 - val_accuracy: 0.9951
    Epoch 25/30
    180/180 [==============================] - 3s 15ms/step - loss: 0.0405 - accuracy: 0.9837 - val_loss: 0.0241 - val_accuracy: 0.9920
    Epoch 26/30
    180/180 [==============================] - 2s 13ms/step - loss: 0.0389 - accuracy: 0.9850 - val_loss: 0.0276 - val_accuracy: 0.9898
    Epoch 27/30
    180/180 [==============================] - 2s 10ms/step - loss: 0.0441 - accuracy: 0.9828 - val_loss: 0.0170 - val_accuracy: 0.9947
    Epoch 28/30
    180/180 [==============================] - 2s 10ms/step - loss: 0.0377 - accuracy: 0.9855 - val_loss: 0.0209 - val_accuracy: 0.9931
    Epoch 29/30
    180/180 [==============================] - 2s 10ms/step - loss: 0.0457 - accuracy: 0.9817 - val_loss: 0.0261 - val_accuracy: 0.9910
    Epoch 30/30
    180/180 [==============================] - 2s 10ms/step - loss: 0.0389 - accuracy: 0.9848 - val_loss: 0.0116 - val_accuracy: 0.9964



```python
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.legend()
```




    <matplotlib.legend.Legend at 0x7f220217d750>




    
![png](/images/output_27_1.png)
    


The model appears to be fully trained after 30 epochs, with a validation accuracy of 99.64% and a training accuracy of 98.48%.

### Model 2: Article Text

We follow the same exact process in creating Model 2, except we change the input to be the article texts instead of the article titles.


```python
vectorize_layer.adapt(train.map(lambda input, output: input["text"]))

text_features = vectorize_layer(text_input)
text_features = layers.Embedding(size_vocabulary, 3, name = "embedding2")(text_features)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.GlobalAveragePooling1D()(text_features)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.Dense(32, activation = "relu")(text_features)
```


```python
main = layers.Dense(32, activation='relu')(text_features)
output = layers.Dense(2, name = "fake")(main)
```


```python
model2 = keras.Model(
    inputs = [text_input],
    outputs = output
)
```


```python
model2.compile(optimizer = "adam",
               loss = losses.SparseCategoricalCrossentropy(from_logits=True),
               metrics=['accuracy']
)
```


```python
history = model2.fit(train, 
                     validation_data=val,
                     epochs = 30)
```

    Epoch 1/30


    /usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:559: UserWarning: Input dict contained keys ['title'] which did not match any model input. They will be ignored by the model.
      inputs = self._flatten_to_reference_inputs(inputs)


    180/180 [==============================] - 4s 17ms/step - loss: 0.6312 - accuracy: 0.6638 - val_loss: 0.3873 - val_accuracy: 0.8856
    Epoch 2/30
    180/180 [==============================] - 4s 24ms/step - loss: 0.2388 - accuracy: 0.9326 - val_loss: 0.1352 - val_accuracy: 0.9618
    Epoch 3/30
    180/180 [==============================] - 5s 30ms/step - loss: 0.1396 - accuracy: 0.9605 - val_loss: 0.1091 - val_accuracy: 0.9696
    Epoch 4/30
    180/180 [==============================] - 5s 25ms/step - loss: 0.1027 - accuracy: 0.9701 - val_loss: 0.0777 - val_accuracy: 0.9764
    Epoch 5/30
    180/180 [==============================] - 3s 18ms/step - loss: 0.0901 - accuracy: 0.9729 - val_loss: 0.0646 - val_accuracy: 0.9851
    Epoch 6/30
    180/180 [==============================] - 3s 16ms/step - loss: 0.0749 - accuracy: 0.9776 - val_loss: 0.0593 - val_accuracy: 0.9860
    Epoch 7/30
    180/180 [==============================] - 3s 16ms/step - loss: 0.0688 - accuracy: 0.9788 - val_loss: 0.0465 - val_accuracy: 0.9885
    Epoch 8/30
    180/180 [==============================] - 3s 16ms/step - loss: 0.0606 - accuracy: 0.9826 - val_loss: 0.0346 - val_accuracy: 0.9928
    Epoch 9/30
    180/180 [==============================] - 3s 16ms/step - loss: 0.0555 - accuracy: 0.9835 - val_loss: 0.0348 - val_accuracy: 0.9913
    Epoch 10/30
    180/180 [==============================] - 3s 16ms/step - loss: 0.0511 - accuracy: 0.9836 - val_loss: 0.0301 - val_accuracy: 0.9947
    Epoch 11/30
    180/180 [==============================] - 3s 16ms/step - loss: 0.0435 - accuracy: 0.9866 - val_loss: 0.0274 - val_accuracy: 0.9940
    Epoch 12/30
    180/180 [==============================] - 3s 16ms/step - loss: 0.0415 - accuracy: 0.9860 - val_loss: 0.0222 - val_accuracy: 0.9944
    Epoch 13/30
    180/180 [==============================] - 3s 16ms/step - loss: 0.0393 - accuracy: 0.9870 - val_loss: 0.0251 - val_accuracy: 0.9949
    Epoch 14/30
    180/180 [==============================] - 3s 16ms/step - loss: 0.0366 - accuracy: 0.9874 - val_loss: 0.0173 - val_accuracy: 0.9951
    Epoch 15/30
    180/180 [==============================] - 3s 16ms/step - loss: 0.0330 - accuracy: 0.9892 - val_loss: 0.0143 - val_accuracy: 0.9976
    Epoch 16/30
    180/180 [==============================] - 3s 16ms/step - loss: 0.0330 - accuracy: 0.9886 - val_loss: 0.0161 - val_accuracy: 0.9971
    Epoch 17/30
    180/180 [==============================] - 3s 16ms/step - loss: 0.0313 - accuracy: 0.9900 - val_loss: 0.0175 - val_accuracy: 0.9978
    Epoch 18/30
    180/180 [==============================] - 3s 16ms/step - loss: 0.0301 - accuracy: 0.9899 - val_loss: 0.0217 - val_accuracy: 0.9969
    Epoch 19/30
    180/180 [==============================] - 3s 17ms/step - loss: 0.0302 - accuracy: 0.9897 - val_loss: 0.0144 - val_accuracy: 0.9980
    Epoch 20/30
    180/180 [==============================] - 3s 17ms/step - loss: 0.0264 - accuracy: 0.9913 - val_loss: 0.0130 - val_accuracy: 0.9978
    Epoch 21/30
    180/180 [==============================] - 3s 17ms/step - loss: 0.0269 - accuracy: 0.9901 - val_loss: 0.0089 - val_accuracy: 0.9984
    Epoch 22/30
    180/180 [==============================] - 3s 17ms/step - loss: 0.0269 - accuracy: 0.9902 - val_loss: 0.0103 - val_accuracy: 0.9978
    Epoch 23/30
    180/180 [==============================] - 3s 16ms/step - loss: 0.0291 - accuracy: 0.9897 - val_loss: 0.0092 - val_accuracy: 0.9978
    Epoch 24/30
    180/180 [==============================] - 3s 16ms/step - loss: 0.0268 - accuracy: 0.9902 - val_loss: 0.0142 - val_accuracy: 0.9973
    Epoch 25/30
    180/180 [==============================] - 3s 16ms/step - loss: 0.0238 - accuracy: 0.9915 - val_loss: 0.0086 - val_accuracy: 0.9987
    Epoch 26/30
    180/180 [==============================] - 3s 16ms/step - loss: 0.0221 - accuracy: 0.9914 - val_loss: 0.0106 - val_accuracy: 0.9976
    Epoch 27/30
    180/180 [==============================] - 3s 16ms/step - loss: 0.0243 - accuracy: 0.9908 - val_loss: 0.0063 - val_accuracy: 0.9993
    Epoch 28/30
    180/180 [==============================] - 3s 17ms/step - loss: 0.0217 - accuracy: 0.9908 - val_loss: 0.0102 - val_accuracy: 0.9984
    Epoch 29/30
    180/180 [==============================] - 3s 16ms/step - loss: 0.0246 - accuracy: 0.9906 - val_loss: 0.0070 - val_accuracy: 0.9991
    Epoch 30/30
    180/180 [==============================] - 3s 16ms/step - loss: 0.0223 - accuracy: 0.9908 - val_loss: 0.0062 - val_accuracy: 0.9993



```python
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.legend()
```




    <matplotlib.legend.Legend at 0x7f22022aa310>




    
![png](/images/output_35_1.png)
    


The model appears to be fully trained after 30 epochs, with a validation accuracy of 99.93% and a training accuracy of 99.08%.

### Model 3: Article Title and Text

In the third model, we `concatenate` the outputs of the `title` and `text` pipelines to create a model which includes both the article titles and texts as inputs. We then compile and plot the model in the same way as before.


```python
main = layers.concatenate([title_features, text_features], axis = 1)
main = layers.Dense(32, activation="relu")(main)

output = layers.Dense(2, name = "fake")(main)
```


```python
model3 = keras.Model(
    inputs = [title_input, text_input],
    outputs = output
)
```


```python
model3.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)
```


```python
history = model3.fit(train, 
                     validation_data=val,
                     epochs = 30)
```

    Epoch 1/30
    180/180 [==============================] - 5s 22ms/step - loss: 0.3217 - accuracy: 0.8999 - val_loss: 0.0817 - val_accuracy: 0.9936
    Epoch 2/30
    180/180 [==============================] - 4s 21ms/step - loss: 0.0675 - accuracy: 0.9877 - val_loss: 0.0349 - val_accuracy: 0.9949
    Epoch 3/30
    180/180 [==============================] - 4s 21ms/step - loss: 0.0373 - accuracy: 0.9922 - val_loss: 0.0232 - val_accuracy: 0.9942
    Epoch 4/30
    180/180 [==============================] - 5s 30ms/step - loss: 0.0247 - accuracy: 0.9940 - val_loss: 0.0145 - val_accuracy: 0.9971
    Epoch 5/30
    180/180 [==============================] - 6s 35ms/step - loss: 0.0199 - accuracy: 0.9953 - val_loss: 0.0064 - val_accuracy: 0.9991
    Epoch 6/30
    180/180 [==============================] - 6s 32ms/step - loss: 0.0171 - accuracy: 0.9953 - val_loss: 0.0042 - val_accuracy: 0.9991
    Epoch 7/30
    180/180 [==============================] - 4s 22ms/step - loss: 0.0126 - accuracy: 0.9965 - val_loss: 0.0044 - val_accuracy: 0.9993
    Epoch 8/30
    180/180 [==============================] - 4s 23ms/step - loss: 0.0130 - accuracy: 0.9960 - val_loss: 0.0060 - val_accuracy: 0.9987
    Epoch 9/30
    180/180 [==============================] - 4s 22ms/step - loss: 0.0115 - accuracy: 0.9963 - val_loss: 0.0036 - val_accuracy: 0.9993
    Epoch 10/30
    180/180 [==============================] - 5s 26ms/step - loss: 0.0102 - accuracy: 0.9967 - val_loss: 0.0047 - val_accuracy: 0.9982
    Epoch 11/30
    180/180 [==============================] - 4s 21ms/step - loss: 0.0092 - accuracy: 0.9972 - val_loss: 0.0031 - val_accuracy: 0.9991
    Epoch 12/30
    180/180 [==============================] - 4s 21ms/step - loss: 0.0099 - accuracy: 0.9963 - val_loss: 0.0015 - val_accuracy: 1.0000
    Epoch 13/30
    180/180 [==============================] - 4s 20ms/step - loss: 0.0084 - accuracy: 0.9972 - val_loss: 0.0042 - val_accuracy: 0.9987
    Epoch 14/30
    180/180 [==============================] - 6s 31ms/step - loss: 0.0071 - accuracy: 0.9979 - val_loss: 0.0040 - val_accuracy: 0.9991
    Epoch 15/30
    180/180 [==============================] - 5s 26ms/step - loss: 0.0072 - accuracy: 0.9970 - val_loss: 0.0020 - val_accuracy: 0.9993
    Epoch 16/30
    180/180 [==============================] - 4s 20ms/step - loss: 0.0056 - accuracy: 0.9982 - val_loss: 0.0015 - val_accuracy: 0.9998
    Epoch 17/30
    180/180 [==============================] - 4s 20ms/step - loss: 0.0067 - accuracy: 0.9980 - val_loss: 6.0378e-04 - val_accuracy: 1.0000
    Epoch 18/30
    180/180 [==============================] - 4s 21ms/step - loss: 0.0067 - accuracy: 0.9979 - val_loss: 6.2756e-04 - val_accuracy: 0.9998
    Epoch 19/30
    180/180 [==============================] - 4s 21ms/step - loss: 0.0056 - accuracy: 0.9980 - val_loss: 3.8568e-04 - val_accuracy: 1.0000
    Epoch 20/30
    180/180 [==============================] - 4s 21ms/step - loss: 0.0050 - accuracy: 0.9983 - val_loss: 0.0021 - val_accuracy: 0.9998
    Epoch 21/30
    180/180 [==============================] - 4s 21ms/step - loss: 0.0070 - accuracy: 0.9975 - val_loss: 0.0010 - val_accuracy: 0.9993
    Epoch 22/30
    180/180 [==============================] - 4s 21ms/step - loss: 0.0078 - accuracy: 0.9972 - val_loss: 0.0026 - val_accuracy: 0.9996
    Epoch 23/30
    180/180 [==============================] - 4s 20ms/step - loss: 0.0060 - accuracy: 0.9978 - val_loss: 8.3377e-04 - val_accuracy: 0.9998
    Epoch 24/30
    180/180 [==============================] - 4s 20ms/step - loss: 0.0062 - accuracy: 0.9978 - val_loss: 2.4190e-04 - val_accuracy: 1.0000
    Epoch 25/30
    180/180 [==============================] - 4s 20ms/step - loss: 0.0043 - accuracy: 0.9987 - val_loss: 3.5416e-04 - val_accuracy: 1.0000
    Epoch 26/30
    180/180 [==============================] - 4s 20ms/step - loss: 0.0062 - accuracy: 0.9977 - val_loss: 3.4553e-04 - val_accuracy: 1.0000
    Epoch 27/30
    180/180 [==============================] - 5s 26ms/step - loss: 0.0044 - accuracy: 0.9984 - val_loss: 0.0014 - val_accuracy: 0.9996
    Epoch 28/30
    180/180 [==============================] - 4s 20ms/step - loss: 0.0064 - accuracy: 0.9979 - val_loss: 4.8349e-04 - val_accuracy: 1.0000
    Epoch 29/30
    180/180 [==============================] - 4s 20ms/step - loss: 0.0052 - accuracy: 0.9978 - val_loss: 0.0012 - val_accuracy: 0.9998
    Epoch 30/30
    180/180 [==============================] - 4s 21ms/step - loss: 0.0075 - accuracy: 0.9973 - val_loss: 2.7188e-04 - val_accuracy: 1.0000



```python
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.legend()
```




    <matplotlib.legend.Legend at 0x7f21803bddd0>




    
![png](/images/output_42_1.png)
    


Again, the model appears to be fully trained after 30 epochs, with a validation accuracy of 100.00% and a training accuracy of 99.73%.

The third model had the best performance in terms of validation accuracy without showing much overfitting, so algorithms should use both the title and the text of articles when seeking to detect fake news.

# 4. Model Evaluation

We can now test our third model's performance on test data that our model has never seen before. We first read in the data and use the `make_dataset` function from above. Then we can fit the model and plot its history.


```python
test_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true"
test_df = pd.read_csv(test_url)
test_df.head()
```





  <div id="df-161ecad9-4a1b-47ef-b7ae-6cfe68069069">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Unnamed: 0</th>
      <th>title</th>
      <th>text</th>
      <th>fake</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>420</td>
      <td>CNN And MSNBC Destroy Trump, Black Out His Fa...</td>
      <td>Donald Trump practically does something to cri...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>14902</td>
      <td>Exclusive: Kremlin tells companies to deliver ...</td>
      <td>The Kremlin wants good news.  The Russian lead...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>322</td>
      <td>Golden State Warriors Coach Just WRECKED Trum...</td>
      <td>On Saturday, the man we re forced to call  Pre...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>16108</td>
      <td>Putin opens monument to Stalin's victims, diss...</td>
      <td>President Vladimir Putin inaugurated a monumen...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>10304</td>
      <td>BREAKING: DNC HACKER FIRED For Bank Fraud…Blam...</td>
      <td>Apparently breaking the law and scamming the g...</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-161ecad9-4a1b-47ef-b7ae-6cfe68069069')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>

  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-161ecad9-4a1b-47ef-b7ae-6cfe68069069 button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-161ecad9-4a1b-47ef-b7ae-6cfe68069069');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>





```python
test_data = make_dataset(test_df)
```


```python
history = model3.fit(test_data, 
                     validation_data=val,
                     epochs = 30,
                     verbose = False)
```


```python
plt.plot(history.history["accuracy"], label = "testing")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.legend()
```




    <matplotlib.legend.Legend at 0x7f218178ff90>




    
![png](/images/output_48_1.png)
    


Our model is about 99.8% accurate on our test data after training for 30 epochs, meaning we are able to detect fake news extremely reliably, more than 99 out of 100 times!

# 5. Embedding Visualization

Lastly, we can use our embedding layer from above to make a visualization. We use Principal Component Analysis to reduce the dimensions of our embedding to a visualizable number.


```python
weights = model3.get_layer('embedding2').get_weights()[0] # get the weights from the embedding layer
vocab = vectorize_layer.get_vocabulary()                # get the vocabulary from our data prep for later

from sklearn.decomposition import PCA
pca = PCA(n_components=2)
weights = pca.fit_transform(weights)

embedding_df = pd.DataFrame({
    'word' : vocab, 
    'x0'   : weights[:,0],
    'x1'   : weights[:,1]
})
```


```python
import plotly.express as px 
fig = px.scatter(embedding_df, 
                 x = "x0", 
                 y = "x1", 
                 size = list(np.ones(len(embedding_df))),
                 size_max = 5,
                 hover_name = "word",
                 color = "x0")

fig.show()

fig.write_html("embedding.html")
```
![png](/images/embedding.png)

Some words we can pick out from the embedding are: 
1. russias
2. GOP
3. build
4. hariri
5. partys

All of these words are far outside of the main circle of points in the middle of the visualization, suggesting that articles with these words in the their text and titles are more likely to contain fake news.

