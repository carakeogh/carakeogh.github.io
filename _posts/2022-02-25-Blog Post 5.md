In this blog post, we will learn how to conduct image classification with TensorFlow, a software library for machine learning and artifical intelligence created by Google.

For our task, we will teach a machine learning algorithm to distinguish betwen pictures of cats and dogs.

# 1. Load Packages and Obtain Data

As always, we start by loading in our packages. The packages that are probably new to you are `tensorflow` and `utils` and `layers` from `tensorflow.keras`. `utils` allows us to perform actions on `numpy` arrays, and we use `layers` in our machine learning model, which we will learn more about later in this post.


```python
import os
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import utils
from tensorflow.keras import layers
import matplotlib.pyplot as plt
import numpy as np
import random
```

For our data, we are going to use a dataset that has labeled images of cats and dogs, and lucky for us, it is already provided by TensorFlow. This code is similar to how we have dealt with data for other machine learning models. In this case, we create a TensorFlow `Dataset` that we will use for training, testing, and validation. Using a `Dataset` lets us build a model using data without actually having to load all the data into memory. We construct this dataset with `image_dataset_from_directory` from `keras` `utils`. There are four different arguments in this function.


1.   this first argument tells the funciton where to look for the images
2.   `shuffle` randomizes the order when the function retrieves the data
3.   `batch_size` is how many pieces of data we retrieve from each data set at once
4.   `image_size` is the size of the input images

We use the function to contruct the training and the validation dataset, and then we construct the test set from the validation dataset. 





```python
# location of data
_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'

# download the data and extract it
path_to_zip = utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)

# construct paths
PATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')

train_dir = os.path.join(PATH, 'train')
validation_dir = os.path.join(PATH, 'validation')

# parameters for datasets
BATCH_SIZE = 32
IMG_SIZE = (160, 160)

# construct train and validation datasets 
train_dataset = utils.image_dataset_from_directory(train_dir,
                                                   shuffle=True,
                                                   batch_size=BATCH_SIZE,
                                                   image_size=IMG_SIZE)

validation_dataset = utils.image_dataset_from_directory(validation_dir,
                                                        shuffle=True,
                                                        batch_size=BATCH_SIZE,
                                                        image_size=IMG_SIZE)

# construct the test dataset by taking every 5th observation out of the validation dataset
val_batches = tf.data.experimental.cardinality(validation_dataset)
test_dataset = validation_dataset.take(val_batches // 5)
validation_dataset = validation_dataset.skip(val_batches // 5)
```

    Downloading data from https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip
    68608000/68606236 [==============================] - 1s 0us/step
    68616192/68606236 [==============================] - 1s 0us/step
    Found 2000 files belonging to 2 classes.
    Found 1000 files belonging to 2 classes.


This function lets us visualize our data, with dogs in the first row and cats in the second row.


```python
def visualize():
  #get the labels (cats, dogs) for the images
  class_names = train_dataset.class_names

  plt.figure(figsize=(10,10))
 
  for images, labels in train_dataset.take(1):
    D = []
    C = []

    #create two lists of the images, separated by cats and dogs 
    for i in range(len(labels)):
      if class_names[labels[i]] == "dogs":
        D.append(i)
      else: 
        C.append(i)

    #take 3 random pictures from each group of images
    rand_D = random.sample(D, k = 3)
    rand_C = random.sample(C, k = 3)

    #plot the images of dogs in the first row and cats in the second row
    for j in range(6):
      ax = plt.subplot(2, 3, j + 1)
      if j <= 2:
        plt.imshow(images[D[j]].numpy().astype("uint8"))
        plt.title("dogs")
        plt.axis("off")
      else:
        plt.imshow(images[C[j-3]].numpy().astype("uint8"))
        plt.title("cats")
        plt.axis("off")
```


```python
visualize()
```


   
![png](/images/output_8_0.png)
    


This code helps us read in the data much more rapidly.


```python
AUTOTUNE = tf.data.AUTOTUNE

train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)
validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)
test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)
```

In order to create a *baseline machine learning model*, which is a model that is makes predictinos based on which label is the most frequent in the data, we create an iterator and compute the proportion of images in the data with labels 0 (cats) and 1 (dogs). 


```python
labels_iterator = train_dataset.unbatch().map(lambda image, label: label).as_numpy_iterator()
```


```python
labels = list(labels_iterator)
```


```python
c = sum(np.array(labels) == 0)
d = sum(np.array(labels) == 1)

print("The number of images in the training data with label '0' (cats) is " + str(c))
print("The number of images in the training data with label '1' (dogs) is " + str(d))
print("The baseline accuracy for the model is " + str(c/(c+d)*100) + "%")
```

    The number of images in the training data with label '0' (cats) is 1000
    The number of images in the training data with label '1' (dogs) is 1000
    The baseline accuracy for the model is 50.0%


The baseline machine learning model would be 50% because the frequency of the labels 0 and 1 are equal. Therefore, the model could randomly guess either cat or dog and be right half of the time.

# 2. First Model

Now we can create our first model. 

A tensor is very similar to a numpy array. Layers make up our models, and they take in one tensor and output another tensor, which could be of a different shape than the original. 

We use `tf.keras.models.Sequential` API because it allows us to easily construct a model by passing a list of layers. 

For our first model, we will use a combination of different layers:
1. `Conv2D`: This layers uses the mathematical operation of convolution. The layers helps us learn what convolutional kernels to use by treating them as parameters and finding out which ones are the most useful to learn the data. We usually use this layer in conjunction with `MaxPooling2D` layers.
2. `MaxPooling2D`: This layer makes the data from the `Conv2D` layer smaller by taking the maximum of a window. The size of this window is determined by the `pool_size` argument, in this case (2, 2).
3. `Dropout`: This layer helps prevent overfitting by randomly setting input units to 0 at a certain frequency, in this case 0.3.
4. `Flatten`: This "flattens" the data from 2-dimensional to 1-dimensional in order to be used in the `Dense` layer. 
5. `Dense`: This is the simplest layer. The first argument, `units`, determines the shape of the new tensor that is output from the layer. The input tensor has `m` rows and `n` columns, and the output tensor has `m` rows and `units` columns. The `activation` argument helps the neural network determine the relationship between input and output values. 


```python
model1 = tf.keras.models.Sequential([
     layers.Conv2D(32, (3, 3), activation = 'relu', input_shape=(160, 160, 3)),
     layers.MaxPooling2D((2, 2)),
     layers.Dropout(0.3),
     layers.Conv2D(64, (3, 3), activation = 'relu'),
     layers.MaxPooling2D((2, 2)), 
     layers.Flatten(),
     layers.Dense(64, activation = 'relu'), 
     layers.Dense(2)
])
```

We can view a summary of the model to see how many parameters were used and the output shape of the tensor from each layer.


```python
model1.summary()
```

    Model: "sequential_4"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     conv2d_8 (Conv2D)           (None, 158, 158, 32)      896       
                                                                     
     max_pooling2d_8 (MaxPooling  (None, 79, 79, 32)       0         
     2D)                                                             
                                                                     
     dropout_4 (Dropout)         (None, 79, 79, 32)        0         
                                                                     
     conv2d_9 (Conv2D)           (None, 77, 77, 64)        18496     
                                                                     
     max_pooling2d_9 (MaxPooling  (None, 38, 38, 64)       0         
     2D)                                                             
                                                                     
     flatten_4 (Flatten)         (None, 92416)             0         
                                                                     
     dense_8 (Dense)             (None, 64)                5914688   
                                                                     
     dense_9 (Dense)             (None, 2)                 130       
                                                                     
    =================================================================
    Total params: 5,934,210
    Trainable params: 5,934,210
    Non-trainable params: 0
    _________________________________________________________________


We compile and fit the model on the training dataset. 


```python
model1.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

history = model1.fit(train_dataset, 
                     epochs=20, 
                     validation_data=validation_dataset)
```

    Epoch 1/20
    63/63 [==============================] - 9s 118ms/step - loss: 143.8583 - accuracy: 0.4945 - val_loss: 0.6934 - val_accuracy: 0.4975
    Epoch 2/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.6859 - accuracy: 0.5390 - val_loss: 0.6934 - val_accuracy: 0.4988
    Epoch 3/20
    63/63 [==============================] - 5s 77ms/step - loss: 0.6515 - accuracy: 0.5970 - val_loss: 0.6947 - val_accuracy: 0.5421
    Epoch 4/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.5763 - accuracy: 0.6585 - val_loss: 0.7437 - val_accuracy: 0.5532
    Epoch 5/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.5379 - accuracy: 0.6915 - val_loss: 0.9029 - val_accuracy: 0.5631
    Epoch 6/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.4642 - accuracy: 0.7505 - val_loss: 0.9884 - val_accuracy: 0.5507
    Epoch 7/20
    63/63 [==============================] - 5s 82ms/step - loss: 0.4039 - accuracy: 0.7915 - val_loss: 1.1118 - val_accuracy: 0.5433
    Epoch 8/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.3634 - accuracy: 0.8220 - val_loss: 1.2642 - val_accuracy: 0.5371
    Epoch 9/20
    63/63 [==============================] - 5s 80ms/step - loss: 0.3292 - accuracy: 0.8345 - val_loss: 1.5327 - val_accuracy: 0.5545
    Epoch 10/20
    63/63 [==============================] - 6s 90ms/step - loss: 0.2700 - accuracy: 0.8625 - val_loss: 1.5941 - val_accuracy: 0.5495
    Epoch 11/20
    63/63 [==============================] - 5s 80ms/step - loss: 0.2163 - accuracy: 0.9020 - val_loss: 1.8193 - val_accuracy: 0.5470
    Epoch 12/20
    63/63 [==============================] - 5s 80ms/step - loss: 0.1966 - accuracy: 0.9095 - val_loss: 1.9226 - val_accuracy: 0.5520
    Epoch 13/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.1568 - accuracy: 0.9230 - val_loss: 2.1957 - val_accuracy: 0.5408
    Epoch 14/20
    63/63 [==============================] - 6s 96ms/step - loss: 0.1173 - accuracy: 0.9505 - val_loss: 2.6532 - val_accuracy: 0.5495
    Epoch 15/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.1268 - accuracy: 0.9500 - val_loss: 2.2307 - val_accuracy: 0.5755
    Epoch 16/20
    63/63 [==============================] - 6s 86ms/step - loss: 0.1314 - accuracy: 0.9510 - val_loss: 3.0954 - val_accuracy: 0.5483
    Epoch 17/20
    63/63 [==============================] - 5s 81ms/step - loss: 0.0899 - accuracy: 0.9585 - val_loss: 2.9494 - val_accuracy: 0.5718
    Epoch 18/20
    63/63 [==============================] - 5s 77ms/step - loss: 0.0947 - accuracy: 0.9680 - val_loss: 2.8124 - val_accuracy: 0.5693
    Epoch 19/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.0825 - accuracy: 0.9685 - val_loss: 3.1477 - val_accuracy: 0.5854
    Epoch 20/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.0965 - accuracy: 0.9645 - val_loss: 3.1463 - val_accuracy: 0.5767


Then, we can plot the validation accuracy compared to the training accuracy.


```python
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```

    
![png](/images/new_output_24_1.png)
    


It is important to experiment with your model to find the parameters that improve the model's accuracy. Some of the parameters I changed in order to do this were: reducing the dropout rate from 0.5 to 0.3 and changing the second Conv2D layer filter from 32 to 64.

**The validation accuracy of the model stabilized between 55% and 60%.** After 20 epochs of training, the validation accuracy of 57.67% with this model outperformed the base model by 7.67% percent. The training accuracy of 96.45% is much higher than the validation accuracy, so this suggests overfitting in model1.

# 3. Model with Data Augmentation

We are going to create a second model by adding *data augmentation layers* to our first model. Data augmentation is when you take an image, modify copies of it, and include it in your training data set, which will help the model in the training process. In this part, we create two layers:
1. `RandomFlip`: randomly flips an image horizontally or vertically
2. `RandomRotation`: randomly rotates an image by the amount factor*2pi.


```python
#create the RandomFlip layer
aug_flip = tf.keras.Sequential([
  tf.keras.layers.RandomFlip('horizontal'),
])

#plot the original and randomly flipped images
for image, blank in train_dataset.take(1):
  plt.figure(figsize=(10,10))
  img = image[0]
  for i in range(9):
    #original image
    if i == 0:  
      ax = plt.subplot(3, 3, i + 1)
      plt.imshow(img / 255)
      plt.title("Original Image")
      plt.axis("off")
    #flipped images
    else:
      ax = plt.subplot(3, 3, i + 1)
      flip_img = aug_flip(tf.expand_dims(img, 0))
      plt.imshow(flip_img[0] / 255)
      plt.axis("off")
```


    
![png](/images/output_28_0.png)
    



```python
#create the RandomRotation layer
aug_rotate = tf.keras.Sequential([
  tf.keras.layers.RandomRotation(0.5)
])

#plot the original and randomly rotated images
for image, blank in train_dataset.take(1):
  plt.figure(figsize=(10,10))
  img = image[0]
  for i in range(9):
    #original image
    if i == 0:  
      ax = plt.subplot(3, 3, i + 1)
      plt.imshow(img / 255)
      plt.title("Original Image")
      plt.axis("off")
    #rotated images
    else:
      ax = plt.subplot(3, 3, i + 1)
      flip_img = aug_rotate(tf.expand_dims(img, 0))
      plt.imshow(flip_img[0] / 255)
      plt.axis("off")

```


    
![png](/images/output_29_0.png)
    


This model user the same layers as model1, but we also add the data augmentation layers described above.


```python
model2 = tf.keras.models.Sequential([
     layers.Conv2D(32, (3, 3), activation = 'relu', input_shape=(160, 160, 3)),
     layers.MaxPooling2D((2, 2)),
     layers.Conv2D(64, (3, 3), activation = 'relu'),
     layers.MaxPooling2D((2, 2)), 
     tf.keras.layers.RandomFlip('horizontal'),
     tf.keras.layers.RandomRotation(0.5),
     layers.Dropout(0.3),
     layers.Flatten(),
     layers.Dense(64, activation = 'relu'), 
     layers.Dense(2)                       
])
```


```python
model2.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

history = model2.fit(train_dataset, 
                    epochs=20, 
                    validation_data=validation_dataset)
```

    Epoch 1/20
    63/63 [==============================] - 9s 83ms/step - loss: 84.0768 - accuracy: 0.5360 - val_loss: 0.7169 - val_accuracy: 0.4765
    Epoch 2/20
    63/63 [==============================] - 5s 80ms/step - loss: 0.7051 - accuracy: 0.4880 - val_loss: 0.6974 - val_accuracy: 0.4802
    Epoch 3/20
    63/63 [==============================] - 5s 82ms/step - loss: 0.7011 - accuracy: 0.5080 - val_loss: 0.7002 - val_accuracy: 0.4876
    Epoch 4/20
    63/63 [==============================] - 5s 81ms/step - loss: 0.6941 - accuracy: 0.5030 - val_loss: 0.6951 - val_accuracy: 0.5074
    Epoch 5/20
    63/63 [==============================] - 6s 92ms/step - loss: 0.6932 - accuracy: 0.5255 - val_loss: 0.6945 - val_accuracy: 0.4926
    Epoch 6/20
    63/63 [==============================] - 7s 98ms/step - loss: 0.6943 - accuracy: 0.5200 - val_loss: 0.6969 - val_accuracy: 0.5124
    Epoch 7/20
    63/63 [==============================] - 6s 90ms/step - loss: 0.7026 - accuracy: 0.5205 - val_loss: 0.7026 - val_accuracy: 0.5334
    Epoch 8/20
    63/63 [==============================] - 6s 83ms/step - loss: 0.6921 - accuracy: 0.5255 - val_loss: 0.6984 - val_accuracy: 0.5124
    Epoch 9/20
    63/63 [==============================] - 6s 86ms/step - loss: 0.6889 - accuracy: 0.5245 - val_loss: 0.6939 - val_accuracy: 0.5557
    Epoch 10/20
    63/63 [==============================] - 6s 89ms/step - loss: 0.6924 - accuracy: 0.5245 - val_loss: 0.6898 - val_accuracy: 0.5111
    Epoch 11/20
    63/63 [==============================] - 6s 88ms/step - loss: 0.6893 - accuracy: 0.5460 - val_loss: 0.7013 - val_accuracy: 0.5582
    Epoch 12/20
    63/63 [==============================] - 6s 90ms/step - loss: 0.6890 - accuracy: 0.5625 - val_loss: 0.7338 - val_accuracy: 0.5532
    Epoch 13/20
    63/63 [==============================] - 7s 108ms/step - loss: 0.6758 - accuracy: 0.5880 - val_loss: 0.7011 - val_accuracy: 0.5334
    Epoch 14/20
    63/63 [==============================] - 6s 87ms/step - loss: 0.6889 - accuracy: 0.5555 - val_loss: 0.6932 - val_accuracy: 0.5532
    Epoch 15/20
    63/63 [==============================] - 6s 90ms/step - loss: 0.6786 - accuracy: 0.5675 - val_loss: 0.6885 - val_accuracy: 0.5297
    Epoch 16/20
    63/63 [==============================] - 6s 92ms/step - loss: 0.6655 - accuracy: 0.6035 - val_loss: 0.7042 - val_accuracy: 0.5619
    Epoch 17/20
    63/63 [==============================] - 6s 93ms/step - loss: 0.6601 - accuracy: 0.6100 - val_loss: 0.6822 - val_accuracy: 0.5842
    Epoch 18/20
    63/63 [==============================] - 6s 96ms/step - loss: 0.6757 - accuracy: 0.6025 - val_loss: 0.6801 - val_accuracy: 0.6114
    Epoch 19/20
    63/63 [==============================] - 7s 112ms/step - loss: 0.6853 - accuracy: 0.5845 - val_loss: 0.6743 - val_accuracy: 0.6101
    Epoch 20/20
    63/63 [==============================] - 6s 89ms/step - loss: 0.6614 - accuracy: 0.6155 - val_loss: 0.6727 - val_accuracy: 0.5978



```python
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```




    <matplotlib.legend.Legend at 0x7fdfe6655450>




    
![png](/images/output_33_1.png)
    


**The validation accuracy of the model during training was 59.78%** The validation accuracy is about 2% higher than that of model1. We do not observe much overfitting in model2 now, as the validation accuracy is now only about 2% lower than the training accuracy.

# 4. Data Preprocessing

We are now going to build our third model, which will add a *preprocessing* layer to our second model. This transforms and simplifies the input data by normalizing the RGB values from a range of 0-255 to a range of 0 to 1.


```python
i = tf.keras.Input(shape=(160, 160, 3))
x = tf.keras.applications.mobilenet_v2.preprocess_input(i)
preprocessor = tf.keras.Model(inputs = [i], outputs = [x])
```


```python
model3 = tf.keras.models.Sequential([
     preprocessor,                                
     layers.Conv2D(64, (3, 3), activation = 'relu', input_shape=(160, 160, 3)),
     layers.MaxPooling2D((2, 2)),
     layers.Conv2D(32, (3, 3), activation = 'relu'),
     layers.MaxPooling2D((2, 2)), 
     tf.keras.layers.RandomFlip('horizontal'),
     tf.keras.layers.RandomRotation(0.5),
     layers.Dropout(0.3),
     layers.Flatten(),
     layers.Dense(64, activation = 'relu'), 
     layers.Dense(2)                       
])
```


```python
model3.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

history = model3.fit(train_dataset, 
                    epochs=20, 
                    validation_data=validation_dataset)
```

    Epoch 1/20
    63/63 [==============================] - 64s 93ms/step - loss: 0.6192 - accuracy: 0.6650 - val_loss: 0.6225 - val_accuracy: 0.6621
    Epoch 2/20
    63/63 [==============================] - 6s 91ms/step - loss: 0.5871 - accuracy: 0.7080 - val_loss: 0.6103 - val_accuracy: 0.6708
    Epoch 3/20
    63/63 [==============================] - 6s 90ms/step - loss: 0.5519 - accuracy: 0.7215 - val_loss: 0.6042 - val_accuracy: 0.6869
    Epoch 4/20
    63/63 [==============================] - 6s 91ms/step - loss: 0.5508 - accuracy: 0.7165 - val_loss: 0.5751 - val_accuracy: 0.7092
    Epoch 5/20
    63/63 [==============================] - 6s 91ms/step - loss: 0.5454 - accuracy: 0.7220 - val_loss: 0.5634 - val_accuracy: 0.7277
    Epoch 6/20
    63/63 [==============================] - 6s 90ms/step - loss: 0.5391 - accuracy: 0.7350 - val_loss: 0.5581 - val_accuracy: 0.7327
    Epoch 7/20
    63/63 [==============================] - 6s 91ms/step - loss: 0.5243 - accuracy: 0.7350 - val_loss: 0.5416 - val_accuracy: 0.7277
    Epoch 8/20
    63/63 [==============================] - 6s 90ms/step - loss: 0.5196 - accuracy: 0.7370 - val_loss: 0.5872 - val_accuracy: 0.7166
    Epoch 9/20
    63/63 [==============================] - 6s 91ms/step - loss: 0.5445 - accuracy: 0.7375 - val_loss: 0.5440 - val_accuracy: 0.7426
    Epoch 10/20
    63/63 [==============================] - 6s 88ms/step - loss: 0.5225 - accuracy: 0.7450 - val_loss: 0.5526 - val_accuracy: 0.7290
    Epoch 11/20
    63/63 [==============================] - 6s 89ms/step - loss: 0.5101 - accuracy: 0.7545 - val_loss: 0.5805 - val_accuracy: 0.7067
    Epoch 12/20
    63/63 [==============================] - 6s 89ms/step - loss: 0.5207 - accuracy: 0.7430 - val_loss: 0.5425 - val_accuracy: 0.7438
    Epoch 13/20
    63/63 [==============================] - 6s 88ms/step - loss: 0.5017 - accuracy: 0.7555 - val_loss: 0.5513 - val_accuracy: 0.7265
    Epoch 14/20
    63/63 [==============================] - 6s 90ms/step - loss: 0.4993 - accuracy: 0.7600 - val_loss: 0.5324 - val_accuracy: 0.7450
    Epoch 15/20
    63/63 [==============================] - 6s 89ms/step - loss: 0.5133 - accuracy: 0.7410 - val_loss: 0.5488 - val_accuracy: 0.7574
    Epoch 16/20
    63/63 [==============================] - 6s 88ms/step - loss: 0.5094 - accuracy: 0.7465 - val_loss: 0.5558 - val_accuracy: 0.7488
    Epoch 17/20
    63/63 [==============================] - 6s 87ms/step - loss: 0.4983 - accuracy: 0.7565 - val_loss: 0.5621 - val_accuracy: 0.7550
    Epoch 18/20
    63/63 [==============================] - 6s 88ms/step - loss: 0.4837 - accuracy: 0.7590 - val_loss: 0.5378 - val_accuracy: 0.7537
    Epoch 19/20
    63/63 [==============================] - 6s 88ms/step - loss: 0.4935 - accuracy: 0.7665 - val_loss: 0.5310 - val_accuracy: 0.7698
    Epoch 20/20
    63/63 [==============================] - 6s 88ms/step - loss: 0.4975 - accuracy: 0.7525 - val_loss: 0.5334 - val_accuracy: 0.7438



```python
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```




    <matplotlib.legend.Legend at 0x7fe0e0188c10>




    
![png](/images/output_40_1.png)
    


**The validation accuracy of the model during training was 74.38%.** The validation accuracy is significantly higher than that of model1. We don't seem to observe overfitting in model3, as the validatiion and training accuracy are within less than 1% of each other.

# 5. Transfer Learning

For our fourth model, we are going to use the concept of *transfer learning* to build upon an existing machine learning model and reduce the number of layers necessary in our model. In order to do this, we access a model that already exists, the "base model". We are using `MobileNetV2`.


```python
IMG_SHAPE = IMG_SIZE + (3,)
base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,
                                               include_top=False,
                                               weights='imagenet')
base_model.trainable = False

i = tf.keras.Input(shape=IMG_SHAPE)
x = base_model(i, training = False)
base_model_layer = tf.keras.Model(inputs = [i], outputs = [x])
```

    Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_160_no_top.h5
    9412608/9406464 [==============================] - 0s 0us/step
    9420800/9406464 [==============================] - 0s 0us/step


We've added the `MobileNetV2` layer, and we don't need all of the layers from before because there is a lot of complexity in the pre-trained base model layer. As you can see in the summary below, we have over 2 million total parameters, with over 350,000 parameters to train in this model.


```python
model4 = tf.keras.models.Sequential([
     preprocessor,
     base_model_layer,
     layers.Conv2D(32, (3, 3), activation='relu', input_shape=(160, 160, 3)),
     layers.MaxPooling2D((3, 3)),    
     layers.Dropout(0.5),                            
     tf.keras.layers.RandomFlip('horizontal'),
     tf.keras.layers.RandomRotation(0.5),
     layers.Flatten(),
     layers.Dense(2, activation = 'relu'),                       
])
```


```python
model4.summary()
```

    Model: "sequential_7"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     model (Functional)          (None, 160, 160, 3)       0         
                                                                     
     model_1 (Functional)        (None, 5, 5, 1280)        2257984   
                                                                     
     conv2d_6 (Conv2D)           (None, 3, 3, 32)          368672    
                                                                     
     max_pooling2d_6 (MaxPooling  (None, 1, 1, 32)         0         
     2D)                                                             
                                                                     
     dropout_3 (Dropout)         (None, 1, 1, 32)          0         
                                                                     
     random_flip_5 (RandomFlip)  (None, 1, 1, 32)          0         
                                                                     
     random_rotation_3 (RandomRo  (None, 1, 1, 32)         0         
     tation)                                                         
                                                                     
     flatten_3 (Flatten)         (None, 32)                0         
                                                                     
     dense_6 (Dense)             (None, 2)                 66        
                                                                     
    =================================================================
    Total params: 2,626,722
    Trainable params: 368,738
    Non-trainable params: 2,257,984
    _________________________________________________________________



```python
model4.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

history = model4.fit(train_dataset, 
                    epochs=20, 
                    validation_data=validation_dataset)
```

    Epoch 1/20
    63/63 [==============================] - 10s 104ms/step - loss: 0.2226 - accuracy: 0.9150 - val_loss: 0.0707 - val_accuracy: 0.9790
    Epoch 2/20
    63/63 [==============================] - 6s 88ms/step - loss: 0.1005 - accuracy: 0.9575 - val_loss: 0.0819 - val_accuracy: 0.9802
    Epoch 3/20
    63/63 [==============================] - 6s 88ms/step - loss: 0.0765 - accuracy: 0.9695 - val_loss: 0.0798 - val_accuracy: 0.9851
    Epoch 4/20
    63/63 [==============================] - 6s 87ms/step - loss: 0.0733 - accuracy: 0.9770 - val_loss: 0.0645 - val_accuracy: 0.9765
    Epoch 5/20
    63/63 [==============================] - 6s 88ms/step - loss: 0.0659 - accuracy: 0.9805 - val_loss: 0.0523 - val_accuracy: 0.9827
    Epoch 6/20
    63/63 [==============================] - 6s 94ms/step - loss: 0.0448 - accuracy: 0.9825 - val_loss: 0.0529 - val_accuracy: 0.9864
    Epoch 7/20
    63/63 [==============================] - 7s 100ms/step - loss: 0.0327 - accuracy: 0.9860 - val_loss: 0.0475 - val_accuracy: 0.9864
    Epoch 8/20
    63/63 [==============================] - 6s 88ms/step - loss: 0.0358 - accuracy: 0.9905 - val_loss: 0.1387 - val_accuracy: 0.9728
    Epoch 9/20
    63/63 [==============================] - 6s 88ms/step - loss: 0.0460 - accuracy: 0.9860 - val_loss: 0.0693 - val_accuracy: 0.9814
    Epoch 10/20
    63/63 [==============================] - 6s 87ms/step - loss: 0.0322 - accuracy: 0.9880 - val_loss: 0.0596 - val_accuracy: 0.9827
    Epoch 11/20
    63/63 [==============================] - 6s 88ms/step - loss: 0.0224 - accuracy: 0.9940 - val_loss: 0.0547 - val_accuracy: 0.9839
    Epoch 12/20
    63/63 [==============================] - 6s 87ms/step - loss: 0.0221 - accuracy: 0.9930 - val_loss: 0.0598 - val_accuracy: 0.9864
    Epoch 13/20
    63/63 [==============================] - 6s 88ms/step - loss: 0.0270 - accuracy: 0.9920 - val_loss: 0.0466 - val_accuracy: 0.9926
    Epoch 14/20
    63/63 [==============================] - 6s 89ms/step - loss: 0.0129 - accuracy: 0.9930 - val_loss: 0.0959 - val_accuracy: 0.9790
    Epoch 15/20
    63/63 [==============================] - 6s 88ms/step - loss: 0.0390 - accuracy: 0.9880 - val_loss: 0.0689 - val_accuracy: 0.9851
    Epoch 16/20
    63/63 [==============================] - 6s 89ms/step - loss: 0.0150 - accuracy: 0.9930 - val_loss: 0.1188 - val_accuracy: 0.9814
    Epoch 17/20
    63/63 [==============================] - 6s 87ms/step - loss: 0.0324 - accuracy: 0.9905 - val_loss: 0.0626 - val_accuracy: 0.9864
    Epoch 18/20
    63/63 [==============================] - 6s 88ms/step - loss: 0.0205 - accuracy: 0.9935 - val_loss: 0.0805 - val_accuracy: 0.9814
    Epoch 19/20
    63/63 [==============================] - 6s 88ms/step - loss: 0.0154 - accuracy: 0.9935 - val_loss: 0.1090 - val_accuracy: 0.9827
    Epoch 20/20
    63/63 [==============================] - 6s 88ms/step - loss: 0.0184 - accuracy: 0.9930 - val_loss: 0.0780 - val_accuracy: 0.9864



```python
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```




    <matplotlib.legend.Legend at 0x7f70ebbcea10>




    
![png](/images/new_output_49_1.png)
    


**The validation accuracy of the model during training was 98.64%.** The validation accuracy is significantly higher than that of all the other models. We don't seem to observe much overfitting in model4, as the validation and training accuracy are close to each other.


# 6. Score on Test Data

Model 4 gives us the best possible validation accuracy, so we will that model on the `test_dataset` and evaluate its accuracy. 


```python
#use model 4 to predict the labels of test data
pred_labels = model4.predict(test_dataset.take(6)).argmax(axis = 1)

plt.figure(figsize=(10,10))
for images, labels in test_dataset.take(5):
  for i in range(20):
    plt.subplot(5, 4, i+1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(images[i].numpy().astype("uint8"))

    if pred_labels[i] == labels[i]:
      plt.xlabel("True")

plt.show()
```
    
![png](model6.png)
    

As you can see, the model 4 achieved 100% accuracy on this test dataset.
